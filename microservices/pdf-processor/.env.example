# ========================================
# PDF Processor Environment Configuration
# ========================================

# Environment Settings
ENVIRONMENT=development
PORT=8001
LOG_LEVEL=INFO
DEBUG=true
TEST_VERSION=1.0.0

# Database
DATABASE_URL=postgresql://audiobooker:password@localhost:5432/audiobooker_db

# Cloudflare R2 Storage
R2_ACCOUNT_ID=your_account_id_here
R2_ACCESS_KEY_ID=your_access_key_here
R2_SECRET_ACCESS_KEY=your_secret_key_here
R2_BUCKET_NAME=your_bucket_name
# R2_ENDPOINT_URL=https://your-account.r2.cloudflarestorage.com  # Optional custom endpoint

# CORS Origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# Processing Configuration
DEFAULT_CHUNK_SIZE=1000
DEFAULT_CHUNK_OVERLAP=200
MAX_FILE_SIZE_MB=100

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
# REDIS_PASSWORD=your_redis_password  # Optional

# ========================================
# LLM Speaker Chunking (Hugging Face Endpoint)
# ========================================
# Hugging Face Inference Endpoint URL
HF_ENDPOINT_URL=https://y23p40p55t1z1xpe.us-east-1.aws.endpoints.huggingface.cloud/v1

# Hugging Face API Token - Required for LLM chunking
HF_TOKEN=hf_your-token-here

# Enable/Disable automatic LLM speaker chunking
ENABLE_LLM_CHUNKING=true

# Model name to use
LLM_MODEL=FruitClamp/qwen-finetuned

# Number of concurrent LLM API requests 
# IMPORTANT: Set to 1 to avoid rate limits (30k TPM limit)
# Only increase if you have a higher tier OpenAI account
LLM_CONCURRENCY=1

# Maximum characters per LLM processing window
# Reduced to ~15000 to stay within rate limits (approx 4k tokens per request)
LLM_MAX_CHARS_PER_WINDOW=15000

# Number of characters to analyze for character discovery
LLM_DISCOVERY_CHARS=20000

# Delay between API requests in seconds (rate limit protection)
LLM_DELAY_BETWEEN_REQUESTS=3.0
